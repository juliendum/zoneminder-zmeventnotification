# Configuration file for object detection

# NOTE: ALL parameters here can be overriden
# on a per monitor basis if you want. Just
# duplicate it inside the correct [monitor-<num>] section

[general]
# This is an optional file
# If specified, you can specify tokens with secret values in that file
# and onlt refer to the tokens in your main config file
secrets = /etc/zm/secrets.ini


# base data path for various files the ES+OD needs
# we support in config variable substitution as well
base_data_path=/var/lib/zmeventnotification

# It seems certain systems don't follow regular
# ZM conventions on install paths. This may cause
# problems with pyzm that the hooks use to do logging
# Look at https://pyzm.readthedocs.io/en/latest/source/pyzm.html#pyzm.ZMLog.init for parameters. Default is "{}"
# You can also use this to control logging irrespective of ZM log settings
#pyzm_overrides = {'conf_path':'/etc/zm'}
pyzm_overrides={'log_level_debug':2}

# base path where ZM config files reside
# this is needed by pyzm especially if your paths are different
# default is /etc/zm
base_zm_conf_path=/etc/zm

# portal/user/password are needed if you plan on using ZM's legacy
# auth mechanism to get images
portal=!ZM_PORTAL
user=!ZM_USER
password=!ZM_PASSWORD

# api portal is needed if you plan to use tokens to get images
# requires ZM 1.33 or above
api_portal=!ZM_API_PORTAL

allow_self_signed=yes
# if yes, last detection will be stored for monitors
# and bounding boxes that match, along with labels
# will be discarded for new detections. This may be helpful
# in getting rid of static objects that get detected
# due to some motion.
match_past_detections=yes
# The max difference in area between the objects if match_past_detection is on
# can also be specified in px like 300px. Default is 5%. Basically, bounding boxes of the same
# object can slightly differ ever so slightly between detection. Contributor @neillbell put in this PR
# to calculate the difference in areas and based on his tests, 5% worked well. YMMV. Change it if needed.
past_det_max_diff_area=5%

# sequence of models to run for detection

models=ml
# if all, then we will loop through all models
# if first then the first success will break out
detection_mode=all

# If you need basic auth to access ZM
#basic_user=user
#basic_password=password



# this is the global detection pattern used for all monitors.
# choose any set of classes from here https://github.com/pjreddie/darknet/blob/master/data/coco.names
# for everything, make it .*
detect_pattern=(person|car|motorbike|bus|truck|boat)
#detect_pattern=.*

# global settings for
# bestmatch, alarm, snapshot OR a specific frame ID
frame_id=bestmatch

# Typically best match means it will first try alarm
# and then snapshot. If you want it the reverse way,
# make the order 's,a'. Don't get imaginative here -
# 's,a' is the only thing it understands. Everything else
# means alarm then snapshot.
#bestmatch_order = 's,a'

# this is the to resize the image before analysis is done
resize=1200
# set to yes, if you want to remove images after analysis
# setting to yes is recommended to avoid filling up space
# keep to no while debugging/inspecting masks
# Note this does NOT delete debug images later
delete_after_analyze=yes

# If yes, will write an image called <filename>-bbox.jpg as well
# which contains the bounding boxes. This has NO relation to
# write_image_to_zm
# Typically, if you enable delete_after_analyze you may
# also want to set  write_debug_image to no.
write_debug_image=no

# if yes, will write an image with bounding boxes
# this needs to be yes to be able to write a bounding box
# image to ZoneMinder that is visible from its console
write_image_to_zm=yes

# Adds percentage to detections
# hog/face shows 100% always
show_percent=yes

# color to be used to draw the polygons you specified
poly_color=(255,255,255)

# If yes, will import zones automatically from monitors
#import_zm_zones=no

# This section gives you an option to get brief animations
# of the event, delivered as part of the push notification to mobile devices
# Animations are created only if an object is detected

[animation]
# Seems like GIF/MP4 animations only
# work in IOS. Too bad.

# NOTE: Animation ONLY works with ZM 1.35 master as of Mar 16, 2020
# You also require zmNinja 1.3.91 or above
# If you are not running that version, animation will not work
# Animation frames will be created, but they won't be pushed to your device

# If yes, object detection will attempt to create
# a short GIF file around the object detection frame
# that can be sent via push notifications for instant playback
# Note this required additional software support. Default:no
create_animation=no

# Format of animation burst
# valid options are "mp4", "gif", "mp4,gif"
# Note that gifs will be of a shorter duration
# as they take up much more disk space than mp4
# Note that if you use mp4, the thumbnail that shows
# with push notifications may look transparent. My guess
# is this is related to how the video is being formed
# in ZM as it is a partial video when we process it

# Note that if you use mp4, you need to change the picture_url
# in zmeventnotification.ini to objdetect_mp4. When you use objdetect,
# a GIF file is checked and if not, the image is returned. MP4 is not
# returned, as they are not playable inside an HTML img tag

animation_types='gif'

# default width of animation image. Be cautious when you increase this
# most mobile platforms give a very brief amount of time (in seconds)
# to download the image.
# Given your ZM instance will be serving the image, it will anyway be slow
# Making the total animation size bigger resulted in the notification not
# getting an image at all (timed out)
animation_width=640

# animation_retry_sleep refers to how long to wait before trying to grab
# frame information if it failed. animation_max_tries defines how many times it
# will try and retrieve frames before it gives up
animation_retry_sleep=15
animation_max_tries=3


[ml]

# Starting version 4.2 of OpenCV, the DNN models support CUDA
# If you have compiled OpenCV 4.2 with CUDA support correctly
# set this to yes. Note that if you have just installed a package
# chances are it is not properly set up with CUDA. It is much better
# you compile OpenCV from source (and uninstall any opencv packages you
# installed via pip or apt-get)
# Read https://www.pyimagesearch.com/2020/02/03/how-to-use-opencvs-dnn-module-with-nvidia-gpus-cuda-and-cudnn/ on how to do it right.
# Play special attention to putting in the right CUDA_ARCH_BIN value that
# matches your GPU or you'll face "invalid device errors in make_policy"
# while trying to actually run it (compile will work fine)

#use_opencv_dnn_cuda=yes

# You can now run the machine learning code on a different server
# This frees up your ZM server for other things
# To do this, you need to setup https://github.com/pliablepixels/mlapi
# on your desired server and confiure it with a user. See its instructions
# once set up, you can choose to do object/face recognition via that
# external serer

# URL that will be used
ml_gateway=http://localohst:5000/api/v1

# If you enable ml_gateway, and it is down
# you can set ml_fallback_local to yes
# if you want to instantiate local object detection
# on gateway failure. Default is no
ml_fallback_local=no

# API/password for remote gateway
ml_user=!ML_USER
ml_password=!ML_PASSWORD


# config params for HOG
[hog]
stride=(4,4)
padding=(8,8)
scale=1.05
mean_shift=-1

[alpr]

# keep this to yes. no mode is not supported today
alpr_use_after_detection_only=yes


# plate_recognizer, open_alpr, open_alpr_cmdline
alpr_service=plate_recognizer

# Many of the ALPR providers offer both a cloud version
# and local SDK version. Sometimes local SDK format differs from
# the cloud instance. Set this to local or cloud. Default cloud
alpr_api_type=cloud

# If you want to host a local SDK https://app.platerecognizer.com/sdk/
#alpr_url=https://localhost:8080/alpr
# Plate recog replace with your api key
alpr_key=!PLATEREC_ALPR_KEY
# if yes, then it will log usage statistics of the ALPR service
platerec_stats=no
# If you want to specify regions. See http://docs.platerecognizer.com/#regions-supported
#platerec_regions=['us','cn','kr']
# minimal confidence for actually detecting a plate
platerec_min_dscore=0.1
# minimal confidence for the translated text
platerec_min_score=0.2


# ----| If you are using openALPR web service |-----
#alpr_service=open_alpr
#alpr_key=!OPENALPR_ALPR_KEY

# For an explanation of params, see http://doc.openalpr.com/api/?api=cloudapi
#openalpr_recognize_vehicle=1
#openalpr_country=us
#openalpr_state=ca
# openalpr returns percents, but we convert to between 0 and 1
#openalpr_min_confidence=0.3


# ----| If you are using openALPR command line |-----

# Before you do any of this, make sure you have openALPR
# compiled and working properly as per http://doc.openalpr.com/compiling.html
# the alpr binary needs to be operational and capable of detecting plates

# Note this is not really very accurate unless you
# have a camera directly with a good view of the palates
# the cloud based API service is far more accurate

#openalpr_cmdline_binary=alpr

# Do an alpr -help to see options, plug them in here
# like say '-j -p ca -c US' etc.
# keep the -j because its JSON

# Note that alpr_pattern is honored
# For the rest, just stuff them in the cmd line options

#openalpr_cmdline_params=-j
#openalpr_cmdline_min_confidence=0.3


